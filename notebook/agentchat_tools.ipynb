{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool using in AutoGen\n",
    "Authors: [Jiale Liu](https://github.com/LeoLjl), [Linxin Song](https://linxins.net/), [Jieyu Zhang](https://jieyuz2.github.io/)\n",
    "\n",
    "In this notebook, we introduce how to use tools in AutoGen. Given a query, a ToolBuilder will retrieve tools based on semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "To use all the tools in the library, we need to install requirements, obtain Bing api key and RapidApi key following the instructions in this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r ../tools/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file. It first looks for an environment variable with a specified name. The value of the environment variable needs to be a valid json string. If that variable is not found, it looks for a json file with the same name. It filters the configs by filter_dict.\n",
    "\n",
    "The config list should look like this:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-4\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },  # Azure OpenAI API endpoint for gpt-4\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },  # Azure OpenAI API endpoint for gpt-4-32k\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Bing\n",
    "In order for web search related tools to operate properly (`perform_web_search`), Bing API is needed. You can read more about how to get an API on the [Bing Web Search API](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api) page.\n",
    "\n",
    "Once you have your key, fill in your key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BING_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure RapidAPI key\n",
    "Some tools in information_retrieval category requires access to RapidAPI. You need to subscribe to these two specific apis in order for related tools to work([link1](https://rapidapi.com/illmagination/api/youtube-captions-and-transcripts/), [link2](https://rapidapi.com/420vijay47/api/youtube-mp3-downloader2/)). These apis have a free pricing tier, no need to worry about extra cost.\n",
    "\n",
    "Once you have the api keys, fill in your key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RAPID_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initate ToolBuilder and retrieve tools\n",
    "Now that all things are set, we should initate a ToolBuilder object and retrieve tools. The ToolBuilder takes in a user query, calculates the semantic similarity between the query and tool description, then retrieves the `topk` amount of tools.\n",
    "\n",
    "Suppose the task we'd like to solve: Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec. What does Teal'c say in response to the question \"Isn't that hot?\"\n",
    "\n",
    "This task requires video transcription skills, so the query can be: Expertise in utilizing YouTube's API or similar services to extract video captions or subtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['information_retrieval get_youtube_caption Retrieves the captions for a YouTube video.', 'information_retrieval youtube_download Downloads a YouTube video and returns the download link.', 'information_retrieval perform_web_search Perform a web search using Bing API.']\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.tool_retriever import ToolBuilder\n",
    "\n",
    "builder = ToolBuilder(\n",
    "    corpus_path=\"../tools/tool_description.tsv\",\n",
    "    retriever=\"all-mpnet-base-v2\",\n",
    ")\n",
    "tool_query = \"Expertise in utilizing YouTube's API or similar services to extract video captions or subtitles.\"\n",
    "tools = builder.retrieve(tool_query, top_k=3)\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get tool signatures and bind it to agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen.tool_utils import get_full_tool_description\n",
    "\n",
    "tool_root = \"../tools\"\n",
    "\n",
    "descriptions = []\n",
    "for tool in tools:\n",
    "    category, tool_name = tool.split(\" \")[0], tool.split(\" \")[1]\n",
    "    tool_path = os.path.join(tool_root, category, f\"{tool_name}.py\")\n",
    "    descriptions.append(get_full_tool_description(tool_path))\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"Information retriever\",\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    "    max_consecutive_auto_reply=5,\n",
    ")\n",
    "proxy = autogen.UserProxyAgent(\n",
    "    name=\"User proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"work_dir\": \"coding\"},  # This will be updated later\n",
    ")\n",
    "\n",
    "# Bind the tools to the assistant\n",
    "builder.bind(assistant, \"\\n\\n\".join(descriptions))\n",
    "\n",
    "# Bind the tools to user proxy\n",
    "proxy = builder.bind_user_proxy(proxy, tool_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the assistant's system message is updated with instructions on how to use tools by writing python code. The user proxy is equipped with executor that can run tool-related code. This feature is based on [User Defined Functions](https://microsoft.github.io/autogen/docs/topics/code-execution/user-defined-functions) and currently cannot operate on Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let the agent finish the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser proxy\u001b[0m (to Information retriever):\n",
      "\n",
      "Today's date is 2024-04-14.\n",
      "# Task\n",
      "You need to solve the below question given by a user.\n",
      "\n",
      "# Question\n",
      "Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec.\n",
      "\n",
      "What does Teal'c say in response to the question \"Isn't that hot?\"\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mInformation retriever\u001b[0m (to User proxy):\n",
      "\n",
      "To solve this task, I will use the `get_youtube_caption` function to retrieve the captions for the YouTube video in question and then search the captions for the line where someone asks \"Isn't that hot?\" and find Teal'c's response to that question. I'll proceed step by step:\n",
      "\n",
      "1. Retrieve the captions of the YouTube video using its video ID (the part of the URL after \"watch?v=\").\n",
      "2. Scan through the captions to find the line containing the question.\n",
      "3. Extract Teal'c's response that immediately follows the question.\n",
      "\n",
      "Let's start with the first step.\n",
      "\n",
      "```python\n",
      "# filename: get_youtube_captions.py\n",
      "from functions import get_youtube_caption\n",
      "\n",
      "# The video ID for the YouTube video\n",
      "video_id = '1htKBjuUWec'\n",
      "\n",
      "# Retrieve the captions of the video\n",
      "captions = get_youtube_caption(video_id)\n",
      "\n",
      "# Print captions to proceed to the next step\n",
      "print(captions)\n",
      "```\n",
      "\n",
      "Please execute the above Python code and provide me with the output of the captions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser proxy\u001b[0m (to Information retriever):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: Wow this coffee's great I was just thinking that yeah is that cinnamon chicory tea oak [Music] isn't that hot extremely\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mInformation retriever\u001b[0m (to User proxy):\n",
      "\n",
      "Based on the provided captions, Teal'c's response to the question \"Isn't that hot?\" is:\n",
      "\n",
      "\"Extremely\"\n",
      "\n",
      "This is the answer you are looking for. There is no need to run further code, as the captions clearly provide Teal'c's response following the relevant question.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"Today's date is 2024-04-14.\n",
    "# Task\n",
    "You need to solve the below question given by a user.\n",
    "\n",
    "# Question\n",
    "{question}\n",
    "\"\"\"\n",
    "question = \"\"\"Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec.\n",
    "\n",
    "What does Teal'c say in response to the question \"Isn't that hot?\"\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = proxy.initiate_chat(assistant, message=PROMPT.format(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the youtube video ourselves, we'll find out that the answer is correct. Provided with relevant apis, language models can do vision and audio related tasks, which can lead to more versatile and useful agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
